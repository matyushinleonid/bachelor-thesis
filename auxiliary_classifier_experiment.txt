Реализовал подход к тренировке классификатора D0 vs D1 с помощью вспомогательного классификатора.

Датасет создал на основе МНИСТа:
C0 - изображения с нулями
С1 - не нули

В целом, размеры классов были такие:
_NUMBER_OF_C0_IN_D0 = 5
_NUMBER_OF_C0_IN_D1 = 900
_NUMBER_OF_C1_IN_D0 = 20000
_NUMBER_OF_C1_IN_D1 = 20000

Разбил на трейн-тест с долей теста в 0.3

Тренировал сначала лог.регрессию 200 эпох, затем тренировал 1 эпоху CNN на лейблах - предсказаниях логрегрессии. В сумме затрачено 105 секунд.
Потом тренировал аналогичную CNN на обычных лейблах D1 2 эпохи. В сумме затрачено 115 секунд.

В конкретном данном случае подход сработал отлично:
0.89 rocauc при тренировке лог.регрессии
0.98 rocauc при тренировке CNN на лейблах - предсказаниях лог.регрессии
0.87 rocauc при тренировке CNN на лейблах D1

Если посмотреть на гистограммы предсказаний этих трёх моделей (лог.регрессии, лог.регрессии+CNN, чистой CNN), то можно увидеть, что чистая CNN препочитает грести все объекты под одну гребёнку (~0.5), и за счёт подобных предсказаний на объектах из C0 rocauc этой модели плохой.
