{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import keras\n",
    "import pandas as pd\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras import backend as K\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from time import time\n",
    "from sklearn.metrics import roc_auc_score as rocauc\n",
    "from sklearn.metrics import accuracy_score as acc\n",
    "from sklearn.model_selection import train_test_split\n",
    "from __future__ import print_function  \n",
    "import seaborn as sns\n",
    "sns.set_style(\"whitegrid\")\n",
    "np.set_printoptions(precision=10)\n",
    "np.set_printoptions(suppress=True)\n",
    "input_shape = (28, 28, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def D1_problem_to_C1_problem(arr, fractions):\n",
    "    number_of_C0_in_D0, number_of_C0_in_D1, number_of_C1_in_D0, number_of_C1_in_D1 = fractions\n",
    "    alpha = number_of_C0_in_D0 / (number_of_C0_in_D0 + number_of_C1_in_D0)\n",
    "    beta = number_of_C0_in_D1 / (number_of_C0_in_D1 + number_of_C1_in_D1)\n",
    "    print((((1 - beta) / (2 - alpha - beta) - beta / (alpha + beta))))\n",
    "    arr = (arr - beta/(alpha + beta))*(((1 - beta) / (2 - alpha - beta) - beta / (alpha + beta)) ** -1)\n",
    "    arr = np.clip(arr,0,1) # значение недообученного классификатора на предыдущем этапе может выйти за границы 0 и 1 \n",
    "    return arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_dataset(C0_values, fractions):\n",
    "    (x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "    x = np.append(x_train, x_test, axis=0)\n",
    "    y = np.append(y_train, y_test)\n",
    "    x = x.reshape(x.shape[0], 28, 28, 1)\n",
    "    y_C1 = np.array([int(elem not in C0_values) for elem in y])\n",
    "\n",
    "    NUMBER_OF_C0_IN_D0, NUMBER_OF_C0_IN_D1, NUMBER_OF_C1_IN_D0, NUMBER_OF_C1_IN_D1 = fractions\n",
    "    i, X, Y_C1, Y_D1 = 0, [], [], []\n",
    "\n",
    "    while (NUMBER_OF_C0_IN_D0, NUMBER_OF_C0_IN_D1, NUMBER_OF_C1_IN_D0, NUMBER_OF_C1_IN_D1) != (0,0,0,0):\n",
    "        if y_C1[i] == 1 and NUMBER_OF_C1_IN_D0 != 0:\n",
    "            Y_C1.append(1)\n",
    "            Y_D1.append(0)\n",
    "            X.append(x[i])\n",
    "            NUMBER_OF_C1_IN_D0 -= 1\n",
    "            i += 1\n",
    "        elif y_C1[i] == 0 and NUMBER_OF_C0_IN_D0 != 0:\n",
    "            Y_C1.append(0)\n",
    "            Y_D1.append(0)\n",
    "            X.append(x[i])\n",
    "            NUMBER_OF_C0_IN_D0 -= 1\n",
    "            i += 1\n",
    "        elif y_C1[i] == 1 and NUMBER_OF_C1_IN_D1 != 0:\n",
    "            Y_C1.append(1)\n",
    "            Y_D1.append(1)\n",
    "            X.append(x[i])\n",
    "            NUMBER_OF_C1_IN_D1 -= 1\n",
    "            i += 1\n",
    "        elif y_C1[i] == 0 and NUMBER_OF_C0_IN_D1 != 0:\n",
    "            Y_C1.append(0)\n",
    "            Y_D1.append(1)\n",
    "            X.append(x[i])\n",
    "            NUMBER_OF_C0_IN_D1 -= 1\n",
    "            i += 1\n",
    "        else:\n",
    "            i += 1\n",
    "    X = np.array(X)\n",
    "    Y_C1 = np.array(Y_C1)\n",
    "    Y_D1 = np.array(Y_D1)\n",
    "    \n",
    "    return train_test_split(X, np.expand_dims(Y_C1, axis=1), np.expand_dims(Y_D1, axis=1), test_size=0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogReg(Sequential):\n",
    "    def __init__(self):\n",
    "        Sequential.__init__(self)\n",
    "        self.add(Flatten(input_shape=input_shape))\n",
    "        self.add(Dense(1, activation='sigmoid'))\n",
    "        self.compile(loss=keras.losses.binary_crossentropy,\n",
    "                   optimizer=keras.optimizers.Adam())\n",
    "        \n",
    "    def fit(self, dataset):\n",
    "        X_train, X_test, Y_C1_train, Y_C1_test, Y_D1_train, Y_D1_test = dataset\n",
    "        \n",
    "        saver_logreg = keras.callbacks.ModelCheckpoint(filepath='/tmp/logreg.hdf5', monitor='val_loss', verbose=0, save_best_only=True)\n",
    "        lr_decreaser_logreg = keras.callbacks.ReduceLROnPlateau(monitor='val_loss', patience=10, factor=0.1, epsilon=0.001, verbose=0)\n",
    "\n",
    "        Sequential.fit(self, X_train, Y_D1_train, validation_data = (X_test, Y_D1_test),\n",
    "                   verbose=0, batch_size=5000, epochs=200, \n",
    "                   callbacks=[saver_logreg, lr_decreaser_logreg])\n",
    "        \n",
    "    def rocauc(self, dataset, fractions):\n",
    "        X_train, X_test, Y_C1_train, Y_C1_test, Y_D1_train, Y_D1_test = dataset\n",
    "        \n",
    "        return rocauc(Y_C1_test, D1_problem_to_C1_problem(self.predict(X_test), fractions).flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Cnn(Sequential):\n",
    "    def __init__(self):\n",
    "        Sequential.__init__(self)\n",
    "        self.add(Conv2D(8, kernel_size=(3, 3),\n",
    "                         activation='relu',\n",
    "                         input_shape=input_shape))\n",
    "        self.add(Conv2D(8, (3, 3), activation='relu'))\n",
    "        self.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "        self.add(Conv2D(16, (3, 3), activation='relu'))\n",
    "        self.add(Conv2D(16, (3, 3), activation='relu'))\n",
    "        self.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "        self.add(Flatten())\n",
    "        self.add(Dense(128, activation='relu'))\n",
    "        self.add(Dense(1, activation='sigmoid'))\n",
    "        self.compile(loss=keras.losses.binary_crossentropy,\n",
    "                                optimizer=keras.optimizers.Adam())\n",
    "        \n",
    "    def fit(self, dataset, epochs, auxiliary_classifier=None):\n",
    "        if auxiliary_classifier is None:\n",
    "            X_train, X_test, Y_C1_train, Y_C1_test, Y_D1_train, Y_D1_test = dataset\n",
    "            name = 'cnn_pure'\n",
    "        else:\n",
    "            X_train, X_test, Y_C1_train, Y_C1_test, _, Y_D1_test = dataset\n",
    "            auxiliary_classifier.load_weights('/tmp/logreg.hdf5')\n",
    "            Y_D1_train = auxiliary_classifier.predict(X_train)\n",
    "            name = 'cnn_aux'\n",
    "        \n",
    "        saver_cnn = keras.callbacks.ModelCheckpoint(filepath='/tmp/'+name, monitor='val_loss', verbose=0, save_best_only=True)\n",
    "        lr_decreaser_cnn = keras.callbacks.ReduceLROnPlateau(monitor='val_loss', patience=3, factor=0.5, epsilon=0.001, verbose=0)\n",
    "        \n",
    "        Sequential.fit(self, X_train, Y_D1_train, validation_data = (X_test, Y_D1_test),\n",
    "                            verbose=1, epochs=epochs,\n",
    "                            callbacks=[saver_cnn, lr_decreaser_cnn])\n",
    "        self.load_weights('/tmp/'+name)\n",
    "        \n",
    "    def rocauc(self, dataset, fractions):\n",
    "        X_train, X_test, Y_C1_train, Y_C1_test, Y_D1_train, Y_D1_test = dataset\n",
    "        \n",
    "        return rocauc(Y_C1_test, D1_problem_to_C1_problem(self.predict(X_test), fractions).flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def experiment(C0_values, fractions):\n",
    "    number_of_C0_in_D0, number_of_C0_in_D1, number_of_C1_in_D0, number_of_C1_in_D1 = fractions\n",
    "    alpha = number_of_C0_in_D0 / (number_of_C0_in_D0 + number_of_C1_in_D0)\n",
    "    beta = number_of_C0_in_D1 / (number_of_C0_in_D1 + number_of_C1_in_D1)\n",
    "    \n",
    "    experiment_logs = {'C0 values': C0_values, 'number of C0 in D0':number_of_C0_in_D0, 'number of C0 in D1':number_of_C0_in_D1, 'number of C1 in D0':number_of_C1_in_D0, 'number of C1 in D1':number_of_C1_in_D1}\n",
    "    experiment_logs['alpha'] = alpha\n",
    "    experiment_logs['beta'] = beta\n",
    "    \n",
    "    dataset = generate_dataset(C0_values, fractions)\n",
    "    logreg, cnn_aux, cnn_pure = LogReg(), Cnn(), Cnn()\n",
    "    \n",
    "    t0 = time()\n",
    "    logreg.fit(dataset)\n",
    "    experiment_logs['logreg time'] = time() - t0\n",
    "    experiment_logs['logreg rocauc'] = logreg.rocauc(dataset, fractions)\n",
    "    \n",
    "    t0 = time()\n",
    "    cnn_aux.fit(dataset, 1, auxiliary_classifier=logreg)\n",
    "    experiment_logs['cnn_aux time'] = time() - t0\n",
    "    experiment_logs['cnn_aux rocauc'] = cnn_aux.rocauc(dataset, fractions)\n",
    "    #cnn_aux.fit(dataset, 1, auxiliary_classifier=logreg)\n",
    "    #experiment_logs['cnn_aux rocauc plus epoch'] = cnn_aux.rocauc(dataset, fractions)\n",
    "    \n",
    "    t0 = time()\n",
    "    cnn_pure.fit(dataset, 2)\n",
    "    experiment_logs['cnn_pure time'] = time() - t0\n",
    "    experiment_logs['cnn_pure rocauc'] = cnn_pure.rocauc(dataset, fractions)\n",
    "    #cnn_pure.fit(dataset, 1)\n",
    "    #experiment_logs['cnn_pure rocauc plus epoch'] = cnn_pure.rocauc(dataset, fractions)\n",
    "    \n",
    "    return experiment_logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "fractions = [[30, 2300, 31000, 31000],\n",
    "             [30, 3000, 31000, 31000]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current experiment:  0\n",
      "-0.5038389572593696\n",
      "Train on 38598 samples, validate on 25732 samples\n",
      "Epoch 1/1\n",
      "38598/38598 [==============================] - 76s - loss: 0.8494 - val_loss: 0.6846\n",
      "-0.5038389572593696\n",
      "Train on 38598 samples, validate on 25732 samples\n",
      "Epoch 1/2\n",
      "38598/38598 [==============================] - 76s - loss: 0.7740 - val_loss: 0.6917\n",
      "Epoch 2/2\n",
      "38598/38598 [==============================] - 76s - loss: 0.6807 - val_loss: 0.6790\n",
      "-0.5038389572593696\n",
      "Current experiment:  1\n",
      "-0.5038389572593696\n",
      "Train on 38598 samples, validate on 25732 samples\n",
      "Epoch 1/1\n",
      "38598/38598 [==============================] - 80s - loss: 0.7268 - val_loss: 0.6846\n",
      "-0.5038389572593696\n",
      "Train on 38598 samples, validate on 25732 samples\n",
      "Epoch 1/2\n",
      "38598/38598 [==============================] - 80s - loss: 0.6992 - val_loss: 0.6815\n",
      "Epoch 2/2\n",
      "38598/38598 [==============================] - 79s - loss: 0.6776 - val_loss: 0.6774\n",
      "-0.5038389572593696\n",
      "Current experiment:  2\n",
      "-0.5038389572593696\n",
      "Train on 38598 samples, validate on 25732 samples\n",
      "Epoch 1/1\n",
      "38598/38598 [==============================] - 80s - loss: 8.3217 - val_loss: 8.3284\n",
      "-0.5038389572593696\n",
      "Train on 38598 samples, validate on 25732 samples\n",
      "Epoch 1/2\n",
      "38598/38598 [==============================] - 80s - loss: 7.6747 - val_loss: 7.7048\n",
      "Epoch 2/2\n",
      "38598/38598 [==============================] - 79s - loss: 7.6800 - val_loss: 7.7048\n",
      "-0.5038389572593696\n",
      "Current experiment:  3\n",
      "-0.5038389572593696\n",
      "Train on 38598 samples, validate on 25732 samples\n",
      "Epoch 1/1\n",
      "38598/38598 [==============================] - 73s - loss: 0.7438 - val_loss: 0.6861\n",
      "-0.5038389572593696\n",
      "Train on 38598 samples, validate on 25732 samples\n",
      "Epoch 1/2\n",
      "38598/38598 [==============================] - 72s - loss: 0.7226 - val_loss: 0.6823\n",
      "Epoch 2/2\n",
      "38598/38598 [==============================] - 71s - loss: 0.6812 - val_loss: 0.6778\n",
      "-0.5038389572593696\n",
      "Current experiment:  4\n",
      "-0.5038389572593696\n",
      "Train on 38598 samples, validate on 25732 samples\n",
      "Epoch 1/1\n",
      "38598/38598 [==============================] - 73s - loss: 0.7553 - val_loss: 0.6859\n",
      "-0.5038389572593696\n",
      "Train on 38598 samples, validate on 25732 samples\n",
      "Epoch 1/2\n",
      "38598/38598 [==============================] - 72s - loss: 0.7283 - val_loss: 0.6825\n",
      "Epoch 2/2\n",
      "38598/38598 [==============================] - 70s - loss: 0.6776 - val_loss: 0.6781\n",
      "-0.5038389572593696\n",
      "Current experiment:  5\n",
      "-0.5038389572593696\n",
      "Train on 38598 samples, validate on 25732 samples\n",
      "Epoch 1/1\n",
      "38598/38598 [==============================] - 74s - loss: 0.7226 - val_loss: 0.6843\n",
      "-0.5038389572593696\n",
      "Train on 38598 samples, validate on 25732 samples\n",
      "Epoch 1/2\n",
      "38598/38598 [==============================] - 74s - loss: 0.7150 - val_loss: 0.6921\n",
      "Epoch 2/2\n",
      "38598/38598 [==============================] - 72s - loss: 0.6911 - val_loss: 0.6893\n",
      "-0.5038389572593696\n",
      "Current experiment:  0\n",
      "-0.5119972337134217\n",
      "Train on 39018 samples, validate on 26012 samples\n",
      "Epoch 1/1\n",
      "39018/39018 [==============================] - 75s - loss: 0.7092 - val_loss: 0.6800\n",
      "-0.5119972337134217\n",
      "Train on 39018 samples, validate on 26012 samples\n",
      "Epoch 1/2\n",
      "39018/39018 [==============================] - 75s - loss: 0.7210 - val_loss: 0.6808\n",
      "Epoch 2/2\n",
      "39018/39018 [==============================] - 73s - loss: 0.6753 - val_loss: 0.6750\n",
      "-0.5119972337134217\n",
      "Current experiment:  1\n",
      "-0.5119972337134217\n",
      "Train on 39018 samples, validate on 26012 samples\n",
      "Epoch 1/1\n",
      "39018/39018 [==============================] - 77s - loss: 0.7166 - val_loss: 0.6807\n",
      "-0.5119972337134217\n",
      "Train on 39018 samples, validate on 26012 samples\n",
      "Epoch 1/2\n",
      "39018/39018 [==============================] - 77s - loss: 0.7397 - val_loss: 0.6786\n",
      "Epoch 2/2\n",
      "39018/39018 [==============================] - 74s - loss: 0.6749 - val_loss: 0.6738\n",
      "-0.5119972337134217\n",
      "Current experiment:  2\n",
      "-0.5119972337134217\n",
      "Train on 39018 samples, validate on 26012 samples\n",
      "Epoch 1/1\n",
      "39018/39018 [==============================] - 77s - loss: 0.7158 - val_loss: 0.6785\n",
      "-0.5119972337134217\n",
      "Train on 39018 samples, validate on 26012 samples\n",
      "Epoch 1/2\n",
      "39018/39018 [==============================] - 76s - loss: 0.7172 - val_loss: 0.6801\n",
      "Epoch 2/2\n",
      "39018/39018 [==============================] - 75s - loss: 0.6735 - val_loss: 0.6718\n",
      "-0.5119972337134217\n",
      "Current experiment:  3\n",
      "-0.5119972337134217\n",
      "Train on 39018 samples, validate on 26012 samples\n",
      "Epoch 1/1\n",
      "39018/39018 [==============================] - 78s - loss: 0.7283 - val_loss: 0.6801\n",
      "-0.5119972337134217\n",
      "Train on 39018 samples, validate on 26012 samples\n",
      "Epoch 1/2\n",
      "39018/39018 [==============================] - 77s - loss: 0.7228 - val_loss: 0.6762\n",
      "Epoch 2/2\n",
      "39018/39018 [==============================] - 75s - loss: 0.6738 - val_loss: 0.6758\n",
      "-0.5119972337134217\n",
      "Current experiment:  4\n",
      "-0.5119972337134217\n",
      "Train on 39018 samples, validate on 26012 samples\n",
      "Epoch 1/1\n",
      "39018/39018 [==============================] - 79s - loss: 0.7285 - val_loss: 0.6798\n",
      "-0.5119972337134217\n",
      "Train on 39018 samples, validate on 26012 samples\n",
      "Epoch 1/2\n",
      "39018/39018 [==============================] - 78s - loss: 0.7015 - val_loss: 0.6740\n",
      "Epoch 2/2\n",
      "39018/39018 [==============================] - 77s - loss: 0.6713 - val_loss: 0.6723\n",
      "-0.5119972337134217\n",
      "Current experiment:  5\n",
      "-0.5119972337134217\n",
      "Train on 39018 samples, validate on 26012 samples\n",
      "Epoch 1/1\n",
      "39018/39018 [==============================] - 79s - loss: 0.7223 - val_loss: 0.6784\n",
      "-0.5119972337134217\n",
      "Train on 39018 samples, validate on 26012 samples\n",
      "Epoch 1/2\n",
      "39018/39018 [==============================] - 79s - loss: 0.7098 - val_loss: 0.6750\n",
      "Epoch 2/2\n",
      "39018/39018 [==============================] - 76s - loss: 0.6709 - val_loss: 0.6702\n",
      "-0.5119972337134217\n"
     ]
    }
   ],
   "source": [
    "logs = []\n",
    "for fraction in fractions:\n",
    "    for i in range(6):\n",
    "        print('Current experiment: ', i)\n",
    "        logs.append(\n",
    "            experiment([0], fraction)\n",
    "            )\n",
    "        pd.DataFrame(logs).to_csv('great_experiment_zero_additional_again.tsv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "fractions = [[30, 60, 27000, 27000],\n",
    "             [30, 120, 27000, 27000],\n",
    "             [30, 210, 27000, 27000],\n",
    "             [30, 330, 27000, 27000],\n",
    "             [30, 480, 27000, 27000],\n",
    "             [30, 630, 27000, 27000],\n",
    "             [30, 800, 27000, 27000],\n",
    "             [30, 1200, 27000, 27000],\n",
    "             [30, 1700, 27000, 27000],\n",
    "             [30, 2300, 27000, 27000],\n",
    "             [30, 3000, 27000, 27000]]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current experiment:  0\n",
      "-0.16669743392833114\n",
      "Train on 32454 samples, validate on 21636 samples\n",
      "Epoch 1/1\n",
      "32454/32454 [==============================] - 61s - loss: 8.0324 - val_loss: 8.0300\n",
      "-0.16669743392833114\n",
      "Train on 32454 samples, validate on 21636 samples\n",
      "Epoch 1/2\n",
      "32454/32454 [==============================] - 62s - loss: 8.0833 - val_loss: 8.0300\n",
      "Epoch 2/2\n",
      "32454/32454 [==============================] - 58s - loss: 8.0859 - val_loss: 8.0300\n",
      "-0.16669743392833114\n",
      "Current experiment:  1\n",
      "-0.16669743392833114\n",
      "Train on 32454 samples, validate on 21636 samples\n",
      "Epoch 1/1\n",
      "32454/32454 [==============================] - 62s - loss: 8.0534 - val_loss: 8.0235\n",
      "-0.16669743392833114\n",
      "Train on 32454 samples, validate on 21636 samples\n",
      "Epoch 1/2\n",
      "32454/32454 [==============================] - 62s - loss: 0.7567 - val_loss: 0.6941\n",
      "Epoch 2/2\n",
      "32454/32454 [==============================] - 60s - loss: 0.6935 - val_loss: 0.6935\n",
      "-0.16669743392833114\n",
      "Current experiment:  2\n",
      "-0.16669743392833114\n",
      "Train on 32454 samples, validate on 21636 samples\n",
      "Epoch 1/1\n",
      "32454/32454 [==============================] - 64s - loss: 0.7260 - val_loss: 0.6940\n",
      "-0.16669743392833114\n",
      "Train on 32454 samples, validate on 21636 samples\n",
      "Epoch 1/2\n",
      "32454/32454 [==============================] - 64s - loss: 0.7691 - val_loss: 0.6948\n",
      "Epoch 2/2\n",
      "32454/32454 [==============================] - 61s - loss: 0.6936 - val_loss: 0.6943\n",
      "-0.16669743392833114\n",
      "Current experiment:  3\n",
      "-0.16669743392833114\n",
      "Train on 32454 samples, validate on 21636 samples\n",
      "Epoch 1/1\n",
      "32454/32454 [==============================] - 66s - loss: 7.9822 - val_loss: 8.0464\n",
      "-0.16669743392833114\n",
      "Train on 32454 samples, validate on 21636 samples\n",
      "Epoch 1/2\n",
      "32454/32454 [==============================] - 66s - loss: 8.1155 - val_loss: 7.9831\n",
      "Epoch 2/2\n",
      "32454/32454 [==============================] - 62s - loss: 8.1172 - val_loss: 7.9831\n",
      "-0.16669743392833114\n",
      "Current experiment:  4\n",
      "-0.16669743392833114\n",
      "Train on 32454 samples, validate on 21636 samples\n",
      "Epoch 1/1\n",
      "32454/32454 [==============================] - 66s - loss: 0.7315 - val_loss: 0.6945s: \n",
      "-0.16669743392833114\n",
      "Train on 32454 samples, validate on 21636 samples\n",
      "Epoch 1/2\n",
      "32454/32454 [==============================] - 67s - loss: 0.7273 - val_loss: 0.6941\n",
      "Epoch 2/2\n",
      "32454/32454 [==============================] - 62s - loss: 0.6932 - val_loss: 0.6930\n",
      "-0.16669743392833114\n",
      "Current experiment:  5\n",
      "-0.16669743392833114\n",
      "Train on 32454 samples, validate on 21636 samples\n",
      "Epoch 1/1\n",
      "32454/32454 [==============================] - 66s - loss: 8.0554 - val_loss: 8.0285\n",
      "-0.16669743392833114\n",
      "Train on 32454 samples, validate on 21636 samples\n",
      "Epoch 1/2\n",
      "32454/32454 [==============================] - 68s - loss: 8.0756 - val_loss: 8.0285\n",
      "Epoch 2/2\n",
      "32454/32454 [==============================] - 64s - loss: 8.0869 - val_loss: 8.0285\n",
      "-0.16669743392833114\n",
      "Current experiment:  0\n",
      "-0.3002986380629574\n",
      "Train on 32490 samples, validate on 21660 samples\n",
      "Epoch 1/1\n",
      "32490/32490 [==============================] - 67s - loss: 0.7575 - val_loss: 0.6954\n",
      "-0.3002986380629574\n",
      "Train on 32490 samples, validate on 21660 samples\n",
      "Epoch 1/2\n",
      "32490/32490 [==============================] - 67s - loss: 0.7915 - val_loss: 0.6955\n",
      "Epoch 2/2\n",
      "32490/32490 [==============================] - 63s - loss: 0.6934 - val_loss: 0.6936\n",
      "-0.3002986380629574\n",
      "Current experiment:  1\n",
      "-0.3002986380629574\n",
      "Train on 32490 samples, validate on 21660 samples\n",
      "Epoch 1/1\n",
      "32490/32490 [==============================] - 67s - loss: 0.7245 - val_loss: 0.6942\n",
      "-0.3002986380629574\n",
      "Train on 32490 samples, validate on 21660 samples\n",
      "Epoch 1/2\n",
      "32490/32490 [==============================] - 68s - loss: 0.7477 - val_loss: 0.6955\n",
      "Epoch 2/2\n",
      "32490/32490 [==============================] - 64s - loss: 0.6937 - val_loss: 0.6946\n",
      "-0.3002986380629574\n",
      "Current experiment:  2\n",
      "-0.3002986380629574\n",
      "Train on 32490 samples, validate on 21660 samples\n",
      "Epoch 1/1\n",
      "32490/32490 [==============================] - 68s - loss: 0.8310 - val_loss: 0.6936\n",
      "-0.3002986380629574\n",
      "Train on 32490 samples, validate on 21660 samples\n",
      "Epoch 1/2\n",
      "32490/32490 [==============================] - 68s - loss: 0.7376 - val_loss: 0.6950\n",
      "Epoch 2/2\n",
      "32490/32490 [==============================] - 62s - loss: 0.6939 - val_loss: 0.6940\n",
      "-0.3002986380629574\n",
      "Current experiment:  3\n",
      "-0.3002986380629574\n",
      "Train on 32490 samples, validate on 21660 samples\n",
      "Epoch 1/1\n",
      "32490/32490 [==============================] - 67s - loss: 0.7867 - val_loss: 0.6939\n",
      "-0.3002986380629574\n",
      "Train on 32490 samples, validate on 21660 samples\n",
      "Epoch 1/2\n",
      "32490/32490 [==============================] - 67s - loss: 0.7158 - val_loss: 0.6931\n",
      "Epoch 2/2\n",
      "32490/32490 [==============================] - 62s - loss: 0.6932 - val_loss: 0.6927\n",
      "-0.3002986380629574\n",
      "Current experiment:  4\n",
      "-0.3002986380629574\n",
      "Train on 32490 samples, validate on 21660 samples\n",
      "Epoch 1/1\n",
      "32490/32490 [==============================] - 67s - loss: 0.7289 - val_loss: 0.6945\n",
      "-0.3002986380629574\n",
      "Train on 32490 samples, validate on 21660 samples\n",
      "Epoch 1/2\n",
      " 3328/32490 [==>...........................] - ETA: 67s - loss: 1.1594"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "logs_new = []\n",
    "for fraction in fractions:\n",
    "    for i in range(6):\n",
    "        print('Current experiment: ', i)\n",
    "        logs_new.append(\n",
    "            experiment([0,1], fraction)\n",
    "            )\n",
    "        pd.DataFrame(logs_new).to_csv('great_experiment_zeroone.tsv', sep='\\t')\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
