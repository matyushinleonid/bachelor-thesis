{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Импортируем необходимые вещи и определяем функции с говорящими названиями"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import keras\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras import backend as K\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from time import time\n",
    "from sklearn.metrics import roc_auc_score as rocauc\n",
    "from sklearn.metrics import accuracy_score as acc\n",
    "from __future__ import print_function  \n",
    "import seaborn as sns\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "np.set_printoptions(precision=10)\n",
    "np.set_printoptions(suppress=True)\n",
    "input_shape = (28, 28, 1)\n",
    "\n",
    "def printimg(tensor):\n",
    "    image = tensor[:,:,0]\n",
    "    plt.imshow(image,aspect=\"auto\")\n",
    "    plt.show()\n",
    "    \n",
    "def D1_problem_to_C1_problem(arr):\n",
    "    arr = (arr - beta/(alpha + beta))*(((1 - beta) / (2 - alpha - beta) - beta / (alpha + beta)) ** -1)\n",
    "    arr = np.clip(arr,0,1) # значение недообученного классификатора на предыдущем этапе может выйти за границы 0 и 1 \n",
    "    return arr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Загружаем датасет:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "x = np.append(x_train, x_test, axis=0)\n",
    "y = np.append(y_train, y_test)\n",
    "x = x.reshape(x.shape[0], 28, 28, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Задаём параметры эксперимента:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "C0_values = [0] # Какие цифры допустимы в классе C_0. В классе C_1 будут все остальные цифры \n",
    "_NUMBER_OF_C0_IN_D0 = 5\n",
    "_NUMBER_OF_C0_IN_D1 = 900\n",
    "_NUMBER_OF_C1_IN_D0 = 20000\n",
    "_NUMBER_OF_C1_IN_D1 = 20000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Выделяем из датасета элементы в необходимых пропорциях, расставляем лейблы:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_C1 = np.array([int(elem not in C0_values) for elem in y])\n",
    "_TOTAL_NUMBER_OF_C0 = len(y) - y_C1.sum()\n",
    "\n",
    "alpha = _NUMBER_OF_C0_IN_D0 / (_NUMBER_OF_C0_IN_D0 + _NUMBER_OF_C1_IN_D0)\n",
    "beta = _NUMBER_OF_C0_IN_D1 / (_NUMBER_OF_C0_IN_D1 + _NUMBER_OF_C1_IN_D1)\n",
    "\n",
    "NUMBER_OF_C0_IN_D0 = _NUMBER_OF_C0_IN_D0\n",
    "NUMBER_OF_C0_IN_D1 = _NUMBER_OF_C0_IN_D1\n",
    "NUMBER_OF_C1_IN_D0 = _NUMBER_OF_C1_IN_D0\n",
    "NUMBER_OF_C1_IN_D1 = _NUMBER_OF_C1_IN_D1\n",
    "\n",
    "y_D1 = []\n",
    "i=0\n",
    "X = []\n",
    "Y_C1 = []\n",
    "Y_D1 = []\n",
    "\n",
    "while (NUMBER_OF_C0_IN_D0, NUMBER_OF_C0_IN_D1, NUMBER_OF_C1_IN_D0, NUMBER_OF_C1_IN_D1) != (0,0,0,0):\n",
    "    if y_C1[i] == 1 and NUMBER_OF_C1_IN_D0 != 0:\n",
    "        Y_C1.append(1)\n",
    "        Y_D1.append(0)\n",
    "        X.append(x[i])\n",
    "        NUMBER_OF_C1_IN_D0 -= 1\n",
    "        i += 1\n",
    "    elif y_C1[i] == 0 and NUMBER_OF_C0_IN_D0 != 0:\n",
    "        Y_C1.append(0)\n",
    "        Y_D1.append(0)\n",
    "        X.append(x[i])\n",
    "        NUMBER_OF_C0_IN_D0 -= 1\n",
    "        i += 1\n",
    "    elif y_C1[i] == 1 and NUMBER_OF_C1_IN_D1 != 0:\n",
    "        Y_C1.append(1)\n",
    "        Y_D1.append(1)\n",
    "        X.append(x[i])\n",
    "        NUMBER_OF_C1_IN_D1 -= 1\n",
    "        i += 1\n",
    "    elif y_C1[i] == 0 and NUMBER_OF_C0_IN_D1 != 0:\n",
    "        Y_C1.append(0)\n",
    "        Y_D1.append(1)\n",
    "        X.append(x[i])\n",
    "        NUMBER_OF_C0_IN_D1 -= 1\n",
    "        i += 1\n",
    "    else:\n",
    "        i += 1\n",
    "X = np.array(X)\n",
    "Y_C1 = np.array(Y_C1)\n",
    "Y_D1 = np.array(Y_D1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Бьём на train/test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, Y_C1_train, Y_C1_test, Y_D1_train, Y_D1_test = train_test_split(X, \n",
    "                                                                                 np.expand_dims(Y_C1, axis=1), \n",
    "                                                                                 np.expand_dims(Y_D1, axis=1), \n",
    "                                                                                 test_size=0.3, \n",
    "                                                                                 random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Определяем лог.регресию и CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "logreg = Sequential()\n",
    "logreg.add(Flatten(input_shape=input_shape))\n",
    "logreg.add(Dense(1, activation='sigmoid'))\n",
    "logreg.compile(loss=keras.losses.binary_crossentropy,\n",
    "               optimizer=keras.optimizers.Adam())\n",
    "\n",
    "cnn = Sequential()\n",
    "cnn.add(Conv2D(8, kernel_size=(3, 3),\n",
    "                 activation='relu',\n",
    "                 input_shape=input_shape))\n",
    "cnn.add(Conv2D(16, (3, 3), activation='relu'))\n",
    "cnn.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "cnn.add(Dropout(0.25))\n",
    "cnn.add(Flatten())\n",
    "cnn.add(Dense(64, activation='relu'))\n",
    "cnn.add(Dropout(0.5))\n",
    "cnn.add(Dense(1, activation='sigmoid'))\n",
    "cnn.compile(loss=keras.losses.binary_crossentropy,\n",
    "                        optimizer=keras.optimizers.Adam())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Учим лог.регрессию"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "t0=time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 28633 samples, validate on 12272 samples\n",
      "Epoch 1/200\n",
      "25000/28633 [=========================>....] - ETA: 0s - loss: 7.7043Epoch 00000: val_loss improved from inf to 7.73144, saving model to /tmp/logreg.hdf5\n",
      "28633/28633 [==============================] - 0s - loss: 7.6987 - val_loss: 7.7314\n",
      "Epoch 2/200\n",
      "25000/28633 [=========================>....] - ETA: 0s - loss: 7.6522Epoch 00001: val_loss improved from 7.73144 to 7.66911, saving model to /tmp/logreg.hdf5\n",
      "28633/28633 [==============================] - 0s - loss: 7.6346 - val_loss: 7.6691\n",
      "Epoch 3/200\n",
      "25000/28633 [=========================>....] - ETA: 0s - loss: 7.5591Epoch 00002: val_loss improved from 7.66911 to 7.61977, saving model to /tmp/logreg.hdf5\n",
      "28633/28633 [==============================] - 0s - loss: 7.5438 - val_loss: 7.6198\n",
      "Epoch 4/200\n",
      "25000/28633 [=========================>....] - ETA: 0s - loss: 7.4861Epoch 00003: val_loss improved from 7.61977 to 7.56578, saving model to /tmp/logreg.hdf5\n",
      "28633/28633 [==============================] - 0s - loss: 7.4625 - val_loss: 7.5658\n",
      "Epoch 5/200\n",
      "25000/28633 [=========================>....] - ETA: 0s - loss: 7.3733Epoch 00004: val_loss improved from 7.56578 to 7.55282, saving model to /tmp/logreg.hdf5\n",
      "28633/28633 [==============================] - 0s - loss: 7.3820 - val_loss: 7.5528\n",
      "Epoch 6/200\n",
      "25000/28633 [=========================>....] - ETA: 0s - loss: 7.3215Epoch 00005: val_loss improved from 7.55282 to 7.43489, saving model to /tmp/logreg.hdf5\n",
      "28633/28633 [==============================] - 0s - loss: 7.3294 - val_loss: 7.4349\n",
      "Epoch 7/200\n",
      "25000/28633 [=========================>....] - ETA: 0s - loss: 7.2081Epoch 00006: val_loss improved from 7.43489 to 7.30921, saving model to /tmp/logreg.hdf5\n",
      "28633/28633 [==============================] - 0s - loss: 7.1881 - val_loss: 7.3092\n",
      "Epoch 8/200\n",
      "25000/28633 [=========================>....] - ETA: 0s - loss: 7.0674Epoch 00007: val_loss improved from 7.30921 to 7.16091, saving model to /tmp/logreg.hdf5\n",
      "28633/28633 [==============================] - 0s - loss: 7.0521 - val_loss: 7.1609\n",
      "Epoch 9/200\n",
      "25000/28633 [=========================>....] - ETA: 0s - loss: 6.9414Epoch 00008: val_loss improved from 7.16091 to 7.00825, saving model to /tmp/logreg.hdf5\n",
      "28633/28633 [==============================] - 0s - loss: 6.9108 - val_loss: 7.0083\n",
      "Epoch 10/200\n",
      "25000/28633 [=========================>....] - ETA: 0s - loss: 6.7890Epoch 00009: val_loss improved from 7.00825 to 6.92366, saving model to /tmp/logreg.hdf5\n",
      "28633/28633 [==============================] - 0s - loss: 6.7858 - val_loss: 6.9237\n",
      "Epoch 11/200\n",
      "25000/28633 [=========================>....] - ETA: 0s - loss: 6.6967Epoch 00010: val_loss improved from 6.92366 to 6.87186, saving model to /tmp/logreg.hdf5\n",
      "28633/28633 [==============================] - 0s - loss: 6.6990 - val_loss: 6.8719\n",
      "Epoch 12/200\n",
      "25000/28633 [=========================>....] - ETA: 0s - loss: 6.6242Epoch 00011: val_loss improved from 6.87186 to 6.78645, saving model to /tmp/logreg.hdf5\n",
      "28633/28633 [==============================] - 0s - loss: 6.5991 - val_loss: 6.7864\n",
      "Epoch 13/200\n",
      "25000/28633 [=========================>....] - ETA: 0s - loss: 6.5268Epoch 00012: val_loss improved from 6.78645 to 6.70208, saving model to /tmp/logreg.hdf5\n",
      "28633/28633 [==============================] - 0s - loss: 6.5283 - val_loss: 6.7021\n",
      "Epoch 14/200\n",
      "25000/28633 [=========================>....] - ETA: 0s - loss: 6.3843Epoch 00013: val_loss improved from 6.70208 to 6.68957, saving model to /tmp/logreg.hdf5\n",
      "28633/28633 [==============================] - 0s - loss: 6.4151 - val_loss: 6.6896\n",
      "Epoch 15/200\n",
      "25000/28633 [=========================>....] - ETA: 0s - loss: 6.3733Epoch 00014: val_loss improved from 6.68957 to 6.55675, saving model to /tmp/logreg.hdf5\n",
      "28633/28633 [==============================] - 0s - loss: 6.3787 - val_loss: 6.5568\n",
      "Epoch 16/200\n",
      "25000/28633 [=========================>....] - ETA: 0s - loss: 6.3279Epoch 00015: val_loss improved from 6.55675 to 6.54368, saving model to /tmp/logreg.hdf5\n",
      "28633/28633 [==============================] - 0s - loss: 6.3095 - val_loss: 6.5437\n",
      "Epoch 17/200\n",
      "25000/28633 [=========================>....] - ETA: 0s - loss: 6.2431Epoch 00016: val_loss improved from 6.54368 to 6.53051, saving model to /tmp/logreg.hdf5\n",
      "28633/28633 [==============================] - 0s - loss: 6.2473 - val_loss: 6.5305\n",
      "Epoch 18/200\n",
      "25000/28633 [=========================>....] - ETA: 0s - loss: 6.1576Epoch 00017: val_loss improved from 6.53051 to 6.46204, saving model to /tmp/logreg.hdf5\n",
      "28633/28633 [==============================] - 0s - loss: 6.1666 - val_loss: 6.4620\n",
      "Epoch 19/200\n",
      "25000/28633 [=========================>....] - ETA: 0s - loss: 6.1222Epoch 00018: val_loss improved from 6.46204 to 6.42948, saving model to /tmp/logreg.hdf5\n",
      "28633/28633 [==============================] - 0s - loss: 6.0989 - val_loss: 6.4295\n",
      "Epoch 20/200\n",
      "25000/28633 [=========================>....] - ETA: 0s - loss: 6.0442Epoch 00019: val_loss improved from 6.42948 to 6.34029, saving model to /tmp/logreg.hdf5\n",
      "28633/28633 [==============================] - 0s - loss: 6.0137 - val_loss: 6.3403\n",
      "Epoch 21/200\n",
      "25000/28633 [=========================>....] - ETA: 0s - loss: 5.9625Epoch 00020: val_loss improved from 6.34029 to 6.25205, saving model to /tmp/logreg.hdf5\n",
      "28633/28633 [==============================] - 0s - loss: 5.9401 - val_loss: 6.2521\n",
      "Epoch 22/200\n",
      "25000/28633 [=========================>....] - ETA: 0s - loss: 5.8657Epoch 00021: val_loss improved from 6.25205 to 6.20789, saving model to /tmp/logreg.hdf5\n",
      "28633/28633 [==============================] - 0s - loss: 5.8973 - val_loss: 6.2079\n",
      "Epoch 23/200\n",
      "25000/28633 [=========================>....] - ETA: 0s - loss: 5.8204Epoch 00022: val_loss improved from 6.20789 to 6.16719, saving model to /tmp/logreg.hdf5\n",
      "28633/28633 [==============================] - 0s - loss: 5.8374 - val_loss: 6.1672\n",
      "Epoch 24/200\n",
      "25000/28633 [=========================>....] - ETA: 0s - loss: 5.7428Epoch 00023: val_loss improved from 6.16719 to 6.11893, saving model to /tmp/logreg.hdf5\n",
      "28633/28633 [==============================] - 0s - loss: 5.7729 - val_loss: 6.1189\n",
      "Epoch 25/200\n",
      "25000/28633 [=========================>....] - ETA: 0s - loss: 5.7339Epoch 00024: val_loss improved from 6.11893 to 6.06290, saving model to /tmp/logreg.hdf5\n",
      "28633/28633 [==============================] - 0s - loss: 5.7390 - val_loss: 6.0629\n",
      "Epoch 26/200\n",
      "25000/28633 [=========================>....] - ETA: 0s - loss: 5.6628Epoch 00025: val_loss improved from 6.06290 to 5.98382, saving model to /tmp/logreg.hdf5\n",
      "28633/28633 [==============================] - 0s - loss: 5.6583 - val_loss: 5.9838\n",
      "Epoch 27/200\n",
      "25000/28633 [=========================>....] - ETA: 0s - loss: 5.6172Epoch 00026: val_loss improved from 5.98382 to 5.93599, saving model to /tmp/logreg.hdf5\n",
      "28633/28633 [==============================] - 0s - loss: 5.5993 - val_loss: 5.9360\n",
      "Epoch 28/200\n",
      "25000/28633 [=========================>....] - ETA: 0s - loss: 5.5144Epoch 00027: val_loss improved from 5.93599 to 5.87883, saving model to /tmp/logreg.hdf5\n",
      "28633/28633 [==============================] - 0s - loss: 5.5285 - val_loss: 5.8788\n",
      "Epoch 29/200\n",
      "25000/28633 [=========================>....] - ETA: 0s - loss: 5.4553Epoch 00028: val_loss improved from 5.87883 to 5.84305, saving model to /tmp/logreg.hdf5\n",
      "28633/28633 [==============================] - 0s - loss: 5.4479 - val_loss: 5.8430\n",
      "Epoch 30/200\n",
      "25000/28633 [=========================>....] - ETA: 0s - loss: 5.4009Epoch 00029: val_loss improved from 5.84305 to 5.78841, saving model to /tmp/logreg.hdf5\n",
      "28633/28633 [==============================] - 0s - loss: 5.3733 - val_loss: 5.7884\n",
      "Epoch 31/200\n",
      "25000/28633 [=========================>....] - ETA: 0s - loss: 5.2785Epoch 00030: val_loss improved from 5.78841 to 5.69114, saving model to /tmp/logreg.hdf5\n",
      "28633/28633 [==============================] - 0s - loss: 5.3008 - val_loss: 5.6911\n",
      "Epoch 32/200\n",
      "25000/28633 [=========================>....] - ETA: 0s - loss: 5.2469Epoch 00031: val_loss improved from 5.69114 to 5.66719, saving model to /tmp/logreg.hdf5\n",
      "28633/28633 [==============================] - 0s - loss: 5.2463 - val_loss: 5.6672\n",
      "Epoch 33/200\n",
      "25000/28633 [=========================>....] - ETA: 0s - loss: 5.1684Epoch 00032: val_loss improved from 5.66719 to 5.54927, saving model to /tmp/logreg.hdf5\n",
      "28633/28633 [==============================] - 0s - loss: 5.1483 - val_loss: 5.5493\n",
      "Epoch 34/200\n",
      "25000/28633 [=========================>....] - ETA: 0s - loss: 5.0781Epoch 00033: val_loss improved from 5.54927 to 5.48921, saving model to /tmp/logreg.hdf5\n",
      "28633/28633 [==============================] - 0s - loss: 5.0573 - val_loss: 5.4892\n",
      "Epoch 35/200\n",
      "25000/28633 [=========================>....] - ETA: 0s - loss: 4.9920Epoch 00034: val_loss improved from 5.48921 to 5.40800, saving model to /tmp/logreg.hdf5\n",
      "28633/28633 [==============================] - 0s - loss: 4.9925 - val_loss: 5.4080\n",
      "Epoch 36/200\n",
      "25000/28633 [=========================>....] - ETA: 0s - loss: 4.9092Epoch 00035: val_loss improved from 5.40800 to 5.30890, saving model to /tmp/logreg.hdf5\n",
      "28633/28633 [==============================] - 0s - loss: 4.9093 - val_loss: 5.3089\n",
      "Epoch 37/200\n",
      "25000/28633 [=========================>....] - ETA: 0s - loss: 4.7979Epoch 00036: val_loss improved from 5.30890 to 5.22024, saving model to /tmp/logreg.hdf5\n",
      "28633/28633 [==============================] - 0s - loss: 4.8134 - val_loss: 5.2202\n",
      "Epoch 38/200\n",
      "20000/28633 [===================>..........] - ETA: 0s - loss: 4.7275Epoch 00037: val_loss improved from 5.22024 to 5.12525, saving model to /tmp/logreg.hdf5\n",
      "28633/28633 [==============================] - 0s - loss: 4.7154 - val_loss: 5.1252\n",
      "Epoch 39/200\n",
      "20000/28633 [===================>..........] - ETA: 0s - loss: 4.6013Epoch 00038: val_loss improved from 5.12525 to 5.02051, saving model to /tmp/logreg.hdf5\n",
      "28633/28633 [==============================] - 0s - loss: 4.6010 - val_loss: 5.0205\n",
      "Epoch 40/200\n",
      "25000/28633 [=========================>....] - ETA: 0s - loss: 4.5292Epoch 00039: val_loss did not improve\n",
      "28633/28633 [==============================] - 0s - loss: 4.5161 - val_loss: 5.0363\n",
      "Epoch 41/200\n",
      "25000/28633 [=========================>....] - ETA: 0s - loss: 4.4893Epoch 00040: val_loss improved from 5.02051 to 4.83939, saving model to /tmp/logreg.hdf5\n",
      "28633/28633 [==============================] - 0s - loss: 4.4873 - val_loss: 4.8394\n",
      "Epoch 42/200\n",
      "25000/28633 [=========================>....] - ETA: 0s - loss: 4.3758Epoch 00041: val_loss improved from 4.83939 to 4.70655, saving model to /tmp/logreg.hdf5\n",
      "28633/28633 [==============================] - 0s - loss: 4.3822 - val_loss: 4.7065\n",
      "Epoch 43/200\n",
      "25000/28633 [=========================>....] - ETA: 0s - loss: 4.2390Epoch 00042: val_loss improved from 4.70655 to 4.62171, saving model to /tmp/logreg.hdf5\n",
      "28633/28633 [==============================] - 0s - loss: 4.2155 - val_loss: 4.6217\n",
      "Epoch 44/200\n",
      "25000/28633 [=========================>....] - ETA: 0s - loss: 4.1021Epoch 00043: val_loss improved from 4.62171 to 4.49686, saving model to /tmp/logreg.hdf5\n",
      "28633/28633 [==============================] - 0s - loss: 4.1079 - val_loss: 4.4969\n",
      "Epoch 45/200\n",
      "25000/28633 [=========================>....] - ETA: 0s - loss: 4.1096Epoch 00044: val_loss improved from 4.49686 to 4.41404, saving model to /tmp/logreg.hdf5\n",
      "28633/28633 [==============================] - 0s - loss: 4.0928 - val_loss: 4.4140\n",
      "Epoch 46/200\n",
      "25000/28633 [=========================>....] - ETA: 0s - loss: 3.9418Epoch 00045: val_loss improved from 4.41404 to 4.25949, saving model to /tmp/logreg.hdf5\n",
      "28633/28633 [==============================] - 0s - loss: 3.9365 - val_loss: 4.2595\n",
      "Epoch 47/200\n",
      "25000/28633 [=========================>....] - ETA: 0s - loss: 3.7965Epoch 00046: val_loss improved from 4.25949 to 4.19136, saving model to /tmp/logreg.hdf5\n",
      "28633/28633 [==============================] - 0s - loss: 3.7833 - val_loss: 4.1914\n",
      "Epoch 48/200\n",
      "25000/28633 [=========================>....] - ETA: 0s - loss: 3.7071Epoch 00047: val_loss improved from 4.19136 to 4.02100, saving model to /tmp/logreg.hdf5\n",
      "28633/28633 [==============================] - 0s - loss: 3.6992 - val_loss: 4.0210\n",
      "Epoch 49/200\n",
      "25000/28633 [=========================>....] - ETA: 0s - loss: 3.5940Epoch 00048: val_loss improved from 4.02100 to 3.97481, saving model to /tmp/logreg.hdf5\n",
      "28633/28633 [==============================] - 0s - loss: 3.5948 - val_loss: 3.9748\n",
      "Epoch 50/200\n",
      "25000/28633 [=========================>....] - ETA: 0s - loss: 3.5075Epoch 00049: val_loss improved from 3.97481 to 3.78212, saving model to /tmp/logreg.hdf5\n",
      "28633/28633 [==============================] - 0s - loss: 3.5068 - val_loss: 3.7821\n",
      "Epoch 51/200\n",
      "25000/28633 [=========================>....] - ETA: 0s - loss: 3.3532Epoch 00050: val_loss improved from 3.78212 to 3.69369, saving model to /tmp/logreg.hdf5\n",
      "28633/28633 [==============================] - 0s - loss: 3.3636 - val_loss: 3.6937\n",
      "Epoch 52/200\n",
      "25000/28633 [=========================>....] - ETA: 0s - loss: 3.2911Epoch 00051: val_loss improved from 3.69369 to 3.55058, saving model to /tmp/logreg.hdf5\n",
      "28633/28633 [==============================] - 0s - loss: 3.3002 - val_loss: 3.5506\n",
      "Epoch 53/200\n",
      "25000/28633 [=========================>....] - ETA: 0s - loss: 3.2139Epoch 00052: val_loss improved from 3.55058 to 3.48249, saving model to /tmp/logreg.hdf5\n",
      "28633/28633 [==============================] - 0s - loss: 3.2330 - val_loss: 3.4825\n",
      "Epoch 54/200\n",
      "25000/28633 [=========================>....] - ETA: 0s - loss: 3.0950Epoch 00053: val_loss improved from 3.48249 to 3.44844, saving model to /tmp/logreg.hdf5\n",
      "28633/28633 [==============================] - 0s - loss: 3.1204 - val_loss: 3.4484\n",
      "Epoch 55/200\n",
      "25000/28633 [=========================>....] - ETA: 0s - loss: 3.0576Epoch 00054: val_loss improved from 3.44844 to 3.44072, saving model to /tmp/logreg.hdf5\n",
      "28633/28633 [==============================] - 0s - loss: 3.0345 - val_loss: 3.4407\n",
      "Epoch 56/200\n",
      "25000/28633 [=========================>....] - ETA: 0s - loss: 3.0224Epoch 00055: val_loss improved from 3.44072 to 3.25085, saving model to /tmp/logreg.hdf5\n",
      "28633/28633 [==============================] - 0s - loss: 2.9986 - val_loss: 3.2508\n",
      "Epoch 57/200\n",
      "25000/28633 [=========================>....] - ETA: 0s - loss: 2.8609Epoch 00056: val_loss improved from 3.25085 to 3.00589, saving model to /tmp/logreg.hdf5\n",
      "28633/28633 [==============================] - 0s - loss: 2.8461 - val_loss: 3.0059\n",
      "Epoch 58/200\n",
      "25000/28633 [=========================>....] - ETA: 0s - loss: 2.7168Epoch 00057: val_loss improved from 3.00589 to 2.91087, saving model to /tmp/logreg.hdf5\n",
      "28633/28633 [==============================] - 0s - loss: 2.7191 - val_loss: 2.9109\n",
      "Epoch 59/200\n",
      "25000/28633 [=========================>....] - ETA: 0s - loss: 2.6498Epoch 00058: val_loss did not improve\n",
      "28633/28633 [==============================] - 0s - loss: 2.6412 - val_loss: 2.9509\n",
      "Epoch 60/200\n",
      "25000/28633 [=========================>....] - ETA: 0s - loss: 2.5589Epoch 00059: val_loss improved from 2.91087 to 2.72842, saving model to /tmp/logreg.hdf5\n",
      "28633/28633 [==============================] - 0s - loss: 2.5587 - val_loss: 2.7284\n",
      "Epoch 61/200\n",
      "20000/28633 [===================>..........] - ETA: 0s - loss: 2.4251Epoch 00060: val_loss did not improve\n",
      "28633/28633 [==============================] - 0s - loss: 2.4663 - val_loss: 2.8947\n",
      "Epoch 62/200\n",
      "25000/28633 [=========================>....] - ETA: 0s - loss: 2.4285Epoch 00061: val_loss improved from 2.72842 to 2.54329, saving model to /tmp/logreg.hdf5\n",
      "28633/28633 [==============================] - 0s - loss: 2.4375 - val_loss: 2.5433\n",
      "Epoch 63/200\n",
      "25000/28633 [=========================>....] - ETA: 0s - loss: 2.4304Epoch 00062: val_loss did not improve\n",
      "28633/28633 [==============================] - 0s - loss: 2.4217 - val_loss: 2.6271\n",
      "Epoch 64/200\n",
      "20000/28633 [===================>..........] - ETA: 0s - loss: 2.3062Epoch 00063: val_loss improved from 2.54329 to 2.45023, saving model to /tmp/logreg.hdf5\n",
      "28633/28633 [==============================] - 0s - loss: 2.2951 - val_loss: 2.4502\n",
      "Epoch 65/200\n",
      "25000/28633 [=========================>....] - ETA: 0s - loss: 2.1402Epoch 00064: val_loss improved from 2.45023 to 2.38847, saving model to /tmp/logreg.hdf5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28633/28633 [==============================] - 0s - loss: 2.1558 - val_loss: 2.3885\n",
      "Epoch 66/200\n",
      "25000/28633 [=========================>....] - ETA: 0s - loss: 2.0861Epoch 00065: val_loss improved from 2.38847 to 2.27637, saving model to /tmp/logreg.hdf5\n",
      "28633/28633 [==============================] - 0s - loss: 2.0784 - val_loss: 2.2764\n",
      "Epoch 67/200\n",
      "25000/28633 [=========================>....] - ETA: 0s - loss: 1.9942Epoch 00066: val_loss improved from 2.27637 to 2.14192, saving model to /tmp/logreg.hdf5\n",
      "28633/28633 [==============================] - 0s - loss: 1.9929 - val_loss: 2.1419\n",
      "Epoch 68/200\n",
      "25000/28633 [=========================>....] - ETA: 0s - loss: 1.9281Epoch 00067: val_loss did not improve\n",
      "28633/28633 [==============================] - 0s - loss: 1.9384 - val_loss: 2.2576\n",
      "Epoch 69/200\n",
      "25000/28633 [=========================>....] - ETA: 0s - loss: 2.0001Epoch 00068: val_loss did not improve\n",
      "28633/28633 [==============================] - 0s - loss: 1.9799 - val_loss: 2.1983\n",
      "Epoch 70/200\n",
      "25000/28633 [=========================>....] - ETA: 0s - loss: 2.0777Epoch 00069: val_loss did not improve\n",
      "28633/28633 [==============================] - 0s - loss: 2.0421 - val_loss: 2.3720\n",
      "Epoch 71/200\n",
      "25000/28633 [=========================>....] - ETA: 0s - loss: 2.0162Epoch 00070: val_loss improved from 2.14192 to 1.96208, saving model to /tmp/logreg.hdf5\n",
      "28633/28633 [==============================] - 0s - loss: 2.0341 - val_loss: 1.9621\n",
      "Epoch 72/200\n",
      "25000/28633 [=========================>....] - ETA: 0s - loss: 1.7830Epoch 00071: val_loss improved from 1.96208 to 1.85647, saving model to /tmp/logreg.hdf5\n",
      "28633/28633 [==============================] - 0s - loss: 1.7729 - val_loss: 1.8565\n",
      "Epoch 73/200\n",
      "25000/28633 [=========================>....] - ETA: 0s - loss: 1.7552Epoch 00072: val_loss improved from 1.85647 to 1.78892, saving model to /tmp/logreg.hdf5\n",
      "28633/28633 [==============================] - 0s - loss: 1.7432 - val_loss: 1.7889\n",
      "Epoch 74/200\n",
      "25000/28633 [=========================>....] - ETA: 0s - loss: 1.6373Epoch 00073: val_loss improved from 1.78892 to 1.74760, saving model to /tmp/logreg.hdf5\n",
      "28633/28633 [==============================] - 0s - loss: 1.6441 - val_loss: 1.7476\n",
      "Epoch 75/200\n",
      "25000/28633 [=========================>....] - ETA: 0s - loss: 1.6462Epoch 00074: val_loss did not improve\n",
      "28633/28633 [==============================] - 0s - loss: 1.6498 - val_loss: 1.7715\n",
      "Epoch 76/200\n",
      "25000/28633 [=========================>....] - ETA: 0s - loss: 1.6383Epoch 00075: val_loss improved from 1.74760 to 1.67674, saving model to /tmp/logreg.hdf5\n",
      "28633/28633 [==============================] - 0s - loss: 1.6465 - val_loss: 1.6767\n",
      "Epoch 77/200\n",
      "25000/28633 [=========================>....] - ETA: 0s - loss: 1.5354Epoch 00076: val_loss did not improve\n",
      "28633/28633 [==============================] - 0s - loss: 1.5442 - val_loss: 1.7230\n",
      "Epoch 78/200\n",
      "25000/28633 [=========================>....] - ETA: 0s - loss: 1.5078Epoch 00077: val_loss improved from 1.67674 to 1.56682, saving model to /tmp/logreg.hdf5\n",
      "28633/28633 [==============================] - 0s - loss: 1.4900 - val_loss: 1.5668\n",
      "Epoch 79/200\n",
      "25000/28633 [=========================>....] - ETA: 0s - loss: 1.4420Epoch 00078: val_loss did not improve\n",
      "28633/28633 [==============================] - 0s - loss: 1.4560 - val_loss: 1.7200\n",
      "Epoch 80/200\n",
      "25000/28633 [=========================>....] - ETA: 0s - loss: 1.5608Epoch 00079: val_loss improved from 1.56682 to 1.49557, saving model to /tmp/logreg.hdf5\n",
      "28633/28633 [==============================] - 0s - loss: 1.5330 - val_loss: 1.4956\n",
      "Epoch 81/200\n",
      "25000/28633 [=========================>....] - ETA: 0s - loss: 1.4153Epoch 00080: val_loss did not improve\n",
      "28633/28633 [==============================] - 0s - loss: 1.4236 - val_loss: 1.5757\n",
      "Epoch 82/200\n",
      "20000/28633 [===================>..........] - ETA: 0s - loss: 1.3439Epoch 00081: val_loss did not improve\n",
      "28633/28633 [==============================] - 0s - loss: 1.3769 - val_loss: 1.5426\n",
      "Epoch 83/200\n",
      "25000/28633 [=========================>....] - ETA: 0s - loss: 1.3676Epoch 00082: val_loss did not improve\n",
      "28633/28633 [==============================] - 0s - loss: 1.3834 - val_loss: 1.5749\n",
      "Epoch 84/200\n",
      "25000/28633 [=========================>....] - ETA: 0s - loss: 1.3578Epoch 00083: val_loss improved from 1.49557 to 1.41804, saving model to /tmp/logreg.hdf5\n",
      "28633/28633 [==============================] - 0s - loss: 1.3472 - val_loss: 1.4180\n",
      "Epoch 85/200\n",
      "25000/28633 [=========================>....] - ETA: 0s - loss: 1.3255Epoch 00084: val_loss improved from 1.41804 to 1.41644, saving model to /tmp/logreg.hdf5\n",
      "28633/28633 [==============================] - 0s - loss: 1.3315 - val_loss: 1.4164\n",
      "Epoch 86/200\n",
      "25000/28633 [=========================>....] - ETA: 0s - loss: 1.2333Epoch 00085: val_loss improved from 1.41644 to 1.31594, saving model to /tmp/logreg.hdf5\n",
      "28633/28633 [==============================] - 0s - loss: 1.2296 - val_loss: 1.3159\n",
      "Epoch 87/200\n",
      "25000/28633 [=========================>....] - ETA: 0s - loss: 1.3352Epoch 00086: val_loss improved from 1.31594 to 1.28603, saving model to /tmp/logreg.hdf5\n",
      "28633/28633 [==============================] - 0s - loss: 1.3374 - val_loss: 1.2860\n",
      "Epoch 88/200\n",
      "25000/28633 [=========================>....] - ETA: 0s - loss: 1.2209Epoch 00087: val_loss did not improve\n",
      "28633/28633 [==============================] - 0s - loss: 1.2619 - val_loss: 1.5132\n",
      "Epoch 89/200\n",
      "25000/28633 [=========================>....] - ETA: 0s - loss: 1.3080Epoch 00088: val_loss did not improve\n",
      "28633/28633 [==============================] - 0s - loss: 1.3328 - val_loss: 1.4485\n",
      "Epoch 90/200\n",
      "25000/28633 [=========================>....] - ETA: 0s - loss: 1.1927Epoch 00089: val_loss improved from 1.28603 to 1.21502, saving model to /tmp/logreg.hdf5\n",
      "28633/28633 [==============================] - 0s - loss: 1.1812 - val_loss: 1.2150\n",
      "Epoch 91/200\n",
      "25000/28633 [=========================>....] - ETA: 0s - loss: 1.1628Epoch 00090: val_loss did not improve\n",
      "28633/28633 [==============================] - 0s - loss: 1.1872 - val_loss: 1.3284\n",
      "Epoch 92/200\n",
      "25000/28633 [=========================>....] - ETA: 0s - loss: 1.1238Epoch 00091: val_loss did not improve\n",
      "28633/28633 [==============================] - 0s - loss: 1.1351 - val_loss: 1.3816\n",
      "Epoch 93/200\n",
      "25000/28633 [=========================>....] - ETA: 0s - loss: 1.2531Epoch 00092: val_loss improved from 1.21502 to 1.17772, saving model to /tmp/logreg.hdf5\n",
      "28633/28633 [==============================] - 0s - loss: 1.2266 - val_loss: 1.1777\n",
      "Epoch 94/200\n",
      "25000/28633 [=========================>....] - ETA: 0s - loss: 1.1852Epoch 00093: val_loss did not improve\n",
      "28633/28633 [==============================] - 0s - loss: 1.1954 - val_loss: 1.2746\n",
      "Epoch 95/200\n",
      "25000/28633 [=========================>....] - ETA: 0s - loss: 1.0744Epoch 00094: val_loss did not improve\n",
      "28633/28633 [==============================] - 0s - loss: 1.0793 - val_loss: 1.1922\n",
      "Epoch 96/200\n",
      "25000/28633 [=========================>....] - ETA: 0s - loss: 1.1362Epoch 00095: val_loss improved from 1.17772 to 1.16667, saving model to /tmp/logreg.hdf5\n",
      "28633/28633 [==============================] - 0s - loss: 1.1358 - val_loss: 1.1667\n",
      "Epoch 97/200\n",
      "25000/28633 [=========================>....] - ETA: 0s - loss: 1.0574Epoch 00096: val_loss did not improve\n",
      "28633/28633 [==============================] - 0s - loss: 1.1029 - val_loss: 1.4268\n",
      "Epoch 98/200\n",
      "25000/28633 [=========================>....] - ETA: 0s - loss: 1.0945Epoch 00097: val_loss did not improve\n",
      "28633/28633 [==============================] - 0s - loss: 1.1074 - val_loss: 1.4112\n",
      "Epoch 99/200\n",
      "25000/28633 [=========================>....] - ETA: 0s - loss: 1.1181Epoch 00098: val_loss did not improve\n",
      "28633/28633 [==============================] - 0s - loss: 1.1283 - val_loss: 1.4727\n",
      "Epoch 100/200\n",
      "25000/28633 [=========================>....] - ETA: 0s - loss: 1.1898Epoch 00099: val_loss improved from 1.16667 to 1.09319, saving model to /tmp/logreg.hdf5\n",
      "28633/28633 [==============================] - 0s - loss: 1.2184 - val_loss: 1.0932\n",
      "Epoch 101/200\n",
      "25000/28633 [=========================>....] - ETA: 0s - loss: 1.0828Epoch 00100: val_loss did not improve\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28633/28633 [==============================] - 0s - loss: 1.0691 - val_loss: 1.3919\n",
      "Epoch 102/200\n",
      "25000/28633 [=========================>....] - ETA: 0s - loss: 1.1184Epoch 00101: val_loss did not improve\n",
      "28633/28633 [==============================] - 0s - loss: 1.1080 - val_loss: 1.2155\n",
      "Epoch 103/200\n",
      "25000/28633 [=========================>....] - ETA: 0s - loss: 1.0440Epoch 00102: val_loss improved from 1.09319 to 0.99646, saving model to /tmp/logreg.hdf5\n",
      "28633/28633 [==============================] - 0s - loss: 1.0292 - val_loss: 0.9965\n",
      "Epoch 104/200\n",
      "25000/28633 [=========================>....] - ETA: 0s - loss: 0.9244Epoch 00103: val_loss did not improve\n",
      "28633/28633 [==============================] - 0s - loss: 0.9300 - val_loss: 1.1240\n",
      "Epoch 105/200\n",
      "25000/28633 [=========================>....] - ETA: 0s - loss: 1.0401Epoch 00104: val_loss did not improve\n",
      "28633/28633 [==============================] - 0s - loss: 1.0191 - val_loss: 1.0401\n",
      "Epoch 106/200\n",
      "25000/28633 [=========================>....] - ETA: 0s - loss: 1.0354Epoch 00105: val_loss did not improve\n",
      "28633/28633 [==============================] - 0s - loss: 1.0229 - val_loss: 1.0195\n",
      "Epoch 107/200\n",
      "25000/28633 [=========================>....] - ETA: 0s - loss: 0.9364Epoch 00106: val_loss improved from 0.99646 to 0.95132, saving model to /tmp/logreg.hdf5\n",
      "28633/28633 [==============================] - 0s - loss: 0.9283 - val_loss: 0.9513\n",
      "Epoch 108/200\n",
      "25000/28633 [=========================>....] - ETA: 0s - loss: 0.8863Epoch 00107: val_loss did not improve\n",
      "28633/28633 [==============================] - 0s - loss: 0.9044 - val_loss: 1.2827\n",
      "Epoch 109/200\n",
      "25000/28633 [=========================>....] - ETA: 0s - loss: 1.0084Epoch 00108: val_loss did not improve\n",
      "28633/28633 [==============================] - 0s - loss: 0.9871 - val_loss: 0.9565\n",
      "Epoch 110/200\n",
      "25000/28633 [=========================>....] - ETA: 0s - loss: 1.0145Epoch 00109: val_loss improved from 0.95132 to 0.93383, saving model to /tmp/logreg.hdf5\n",
      "28633/28633 [==============================] - 0s - loss: 1.0062 - val_loss: 0.9338\n",
      "Epoch 111/200\n",
      "25000/28633 [=========================>....] - ETA: 0s - loss: 0.8730Epoch 00110: val_loss did not improve\n",
      "28633/28633 [==============================] - 0s - loss: 0.8857 - val_loss: 1.0726\n",
      "Epoch 112/200\n",
      "25000/28633 [=========================>....] - ETA: 0s - loss: 0.9561Epoch 00111: val_loss did not improve\n",
      "28633/28633 [==============================] - 0s - loss: 0.9576 - val_loss: 1.1218\n",
      "Epoch 113/200\n",
      "25000/28633 [=========================>....] - ETA: 0s - loss: 0.9614Epoch 00112: val_loss did not improve\n",
      "28633/28633 [==============================] - 0s - loss: 0.9479 - val_loss: 1.0130\n",
      "Epoch 114/200\n",
      "25000/28633 [=========================>....] - ETA: 0s - loss: 0.9581Epoch 00113: val_loss improved from 0.93383 to 0.89168, saving model to /tmp/logreg.hdf5\n",
      "28633/28633 [==============================] - 0s - loss: 0.9422 - val_loss: 0.8917\n",
      "Epoch 115/200\n",
      "25000/28633 [=========================>....] - ETA: 0s - loss: 0.9374Epoch 00114: val_loss did not improve\n",
      "28633/28633 [==============================] - 0s - loss: 0.9437 - val_loss: 0.9935\n",
      "Epoch 116/200\n",
      "25000/28633 [=========================>....] - ETA: 0s - loss: 0.8388Epoch 00115: val_loss improved from 0.89168 to 0.87142, saving model to /tmp/logreg.hdf5\n",
      "28633/28633 [==============================] - 0s - loss: 0.8376 - val_loss: 0.8714\n",
      "Epoch 117/200\n",
      "25000/28633 [=========================>....] - ETA: 0s - loss: 0.8565Epoch 00116: val_loss did not improve\n",
      "28633/28633 [==============================] - 0s - loss: 0.8858 - val_loss: 1.2624\n",
      "Epoch 118/200\n",
      "25000/28633 [=========================>....] - ETA: 0s - loss: 0.9275Epoch 00117: val_loss did not improve\n",
      "28633/28633 [==============================] - 0s - loss: 0.9183 - val_loss: 1.0638\n",
      "Epoch 119/200\n",
      "25000/28633 [=========================>....] - ETA: 0s - loss: 0.9576Epoch 00118: val_loss did not improve\n",
      "28633/28633 [==============================] - 0s - loss: 0.9417 - val_loss: 1.0422\n",
      "Epoch 120/200\n",
      "25000/28633 [=========================>....] - ETA: 0s - loss: 0.9866Epoch 00119: val_loss did not improve\n",
      "28633/28633 [==============================] - 0s - loss: 0.9623 - val_loss: 0.8918\n",
      "Epoch 121/200\n",
      "25000/28633 [=========================>....] - ETA: 0s - loss: 0.9503Epoch 00120: val_loss improved from 0.87142 to 0.85132, saving model to /tmp/logreg.hdf5\n",
      "28633/28633 [==============================] - 0s - loss: 0.9330 - val_loss: 0.8513\n",
      "Epoch 122/200\n",
      "25000/28633 [=========================>....] - ETA: 0s - loss: 0.7935Epoch 00121: val_loss did not improve\n",
      "28633/28633 [==============================] - 0s - loss: 0.8035 - val_loss: 0.9476\n",
      "Epoch 123/200\n",
      "25000/28633 [=========================>....] - ETA: 0s - loss: 0.8958Epoch 00122: val_loss did not improve\n",
      "28633/28633 [==============================] - 0s - loss: 0.8832 - val_loss: 0.8679\n",
      "Epoch 124/200\n",
      "25000/28633 [=========================>....] - ETA: 0s - loss: 0.8557Epoch 00123: val_loss did not improve\n",
      "28633/28633 [==============================] - 0s - loss: 0.8590 - val_loss: 0.9199\n",
      "Epoch 125/200\n",
      "25000/28633 [=========================>....] - ETA: 0s - loss: 0.8210Epoch 00124: val_loss did not improve\n",
      "28633/28633 [==============================] - 0s - loss: 0.8137 - val_loss: 0.8874\n",
      "Epoch 126/200\n",
      "25000/28633 [=========================>....] - ETA: 0s - loss: 0.9461Epoch 00125: val_loss improved from 0.85132 to 0.83884, saving model to /tmp/logreg.hdf5\n",
      "28633/28633 [==============================] - 0s - loss: 0.9296 - val_loss: 0.8388\n",
      "Epoch 127/200\n",
      "25000/28633 [=========================>....] - ETA: 0s - loss: 0.7725Epoch 00126: val_loss did not improve\n",
      "28633/28633 [==============================] - 0s - loss: 0.7913 - val_loss: 1.1376\n",
      "Epoch 128/200\n",
      "25000/28633 [=========================>....] - ETA: 0s - loss: 0.8881Epoch 00127: val_loss did not improve\n",
      "28633/28633 [==============================] - 0s - loss: 0.8858 - val_loss: 1.1530\n",
      "Epoch 129/200\n",
      "25000/28633 [=========================>....] - ETA: 0s - loss: 0.9082Epoch 00128: val_loss did not improve\n",
      "28633/28633 [==============================] - 0s - loss: 0.8973 - val_loss: 1.0033\n",
      "Epoch 130/200\n",
      "25000/28633 [=========================>....] - ETA: 0s - loss: 0.9206Epoch 00129: val_loss improved from 0.83884 to 0.83152, saving model to /tmp/logreg.hdf5\n",
      "28633/28633 [==============================] - 0s - loss: 0.8982 - val_loss: 0.8315\n",
      "Epoch 131/200\n",
      "25000/28633 [=========================>....] - ETA: 0s - loss: 0.7715Epoch 00130: val_loss improved from 0.83152 to 0.80438, saving model to /tmp/logreg.hdf5\n",
      "28633/28633 [==============================] - 0s - loss: 0.7668 - val_loss: 0.8044\n",
      "Epoch 132/200\n",
      "25000/28633 [=========================>....] - ETA: 0s - loss: 0.8755Epoch 00131: val_loss did not improve\n",
      "28633/28633 [==============================] - 0s - loss: 0.8746 - val_loss: 0.8685\n",
      "Epoch 133/200\n",
      "25000/28633 [=========================>....] - ETA: 0s - loss: 0.8047Epoch 00132: val_loss did not improve\n",
      "28633/28633 [==============================] - 0s - loss: 0.8097 - val_loss: 0.9231\n",
      "Epoch 134/200\n",
      "25000/28633 [=========================>....] - ETA: 0s - loss: 0.8265Epoch 00133: val_loss did not improve\n",
      "28633/28633 [==============================] - 0s - loss: 0.8203 - val_loss: 0.8295\n",
      "Epoch 135/200\n",
      "25000/28633 [=========================>....] - ETA: 0s - loss: 0.8739Epoch 00134: val_loss did not improve\n",
      "28633/28633 [==============================] - 0s - loss: 0.8741 - val_loss: 0.8661\n",
      "Epoch 136/200\n",
      "25000/28633 [=========================>....] - ETA: 0s - loss: 0.7649Epoch 00135: val_loss improved from 0.80438 to 0.78805, saving model to /tmp/logreg.hdf5\n",
      "28633/28633 [==============================] - 0s - loss: 0.7613 - val_loss: 0.7880\n",
      "Epoch 137/200\n",
      "25000/28633 [=========================>....] - ETA: 0s - loss: 0.7728Epoch 00136: val_loss did not improve\n",
      "28633/28633 [==============================] - 0s - loss: 0.7980 - val_loss: 0.9844\n",
      "Epoch 138/200\n",
      "25000/28633 [=========================>....] - ETA: 0s - loss: 0.7925Epoch 00137: val_loss improved from 0.78805 to 0.77673, saving model to /tmp/logreg.hdf5\n",
      "28633/28633 [==============================] - 0s - loss: 0.7847 - val_loss: 0.7767\n",
      "Epoch 139/200\n",
      "25000/28633 [=========================>....] - ETA: 0s - loss: 0.7211Epoch 00138: val_loss improved from 0.77673 to 0.77514, saving model to /tmp/logreg.hdf5\n",
      "28633/28633 [==============================] - 0s - loss: 0.7230 - val_loss: 0.7751\n",
      "Epoch 140/200\n",
      "25000/28633 [=========================>....] - ETA: 0s - loss: 0.8199Epoch 00139: val_loss did not improve\n",
      "28633/28633 [==============================] - 0s - loss: 0.8343 - val_loss: 0.8674\n",
      "Epoch 141/200\n",
      "25000/28633 [=========================>....] - ETA: 0s - loss: 0.7709Epoch 00140: val_loss did not improve\n",
      "28633/28633 [==============================] - 0s - loss: 0.7841 - val_loss: 1.0643\n",
      "Epoch 142/200\n",
      "25000/28633 [=========================>....] - ETA: 0s - loss: 0.8429Epoch 00141: val_loss did not improve\n",
      "28633/28633 [==============================] - 0s - loss: 0.8312 - val_loss: 0.9712\n",
      "Epoch 143/200\n",
      "25000/28633 [=========================>....] - ETA: 0s - loss: 0.8851Epoch 00142: val_loss did not improve\n",
      "28633/28633 [==============================] - 0s - loss: 0.8909 - val_loss: 1.1070\n",
      "Epoch 144/200\n",
      "25000/28633 [=========================>....] - ETA: 0s - loss: 0.8403Epoch 00143: val_loss did not improve\n",
      "28633/28633 [==============================] - 0s - loss: 0.8323 - val_loss: 0.8749\n",
      "Epoch 145/200\n",
      "25000/28633 [=========================>....] - ETA: 0s - loss: 0.8304Epoch 00144: val_loss improved from 0.77514 to 0.76763, saving model to /tmp/logreg.hdf5\n",
      "28633/28633 [==============================] - 0s - loss: 0.8171 - val_loss: 0.7676\n",
      "Epoch 146/200\n",
      "25000/28633 [=========================>....] - ETA: 0s - loss: 0.7160Epoch 00145: val_loss did not improve\n",
      "28633/28633 [==============================] - 0s - loss: 0.7208 - val_loss: 0.8291\n",
      "Epoch 147/200\n",
      "25000/28633 [=========================>....] - ETA: 0s - loss: 0.7840Epoch 00146: val_loss did not improve\n",
      "28633/28633 [==============================] - 0s - loss: 0.7854 - val_loss: 0.8631\n",
      "Epoch 148/200\n",
      "25000/28633 [=========================>....] - ETA: 0s - loss: 0.8242Epoch 00147: val_loss did not improve\n",
      "28633/28633 [==============================] - 0s - loss: 0.8249 - val_loss: 0.8438\n",
      "Epoch 149/200\n",
      "25000/28633 [=========================>....] - ETA: 0s - loss: 0.7992Epoch 00148: val_loss did not improve\n",
      "28633/28633 [==============================] - 0s - loss: 0.8148 - val_loss: 0.9159\n",
      "Epoch 150/200\n",
      "25000/28633 [=========================>....] - ETA: 0s - loss: 0.8106Epoch 00149: val_loss did not improve\n",
      "28633/28633 [==============================] - 0s - loss: 0.8015 - val_loss: 0.7735\n",
      "Epoch 151/200\n",
      "25000/28633 [=========================>....] - ETA: 0s - loss: 0.7100Epoch 00150: val_loss did not improve\n",
      "28633/28633 [==============================] - 0s - loss: 0.7223 - val_loss: 0.9726\n",
      "Epoch 152/200\n",
      "25000/28633 [=========================>....] - ETA: 0s - loss: 0.8463Epoch 00151: val_loss did not improve\n",
      "28633/28633 [==============================] - 0s - loss: 0.8537 - val_loss: 1.1228\n",
      "Epoch 153/200\n",
      "25000/28633 [=========================>....] - ETA: 0s - loss: 0.8058Epoch 00152: val_loss did not improve\n",
      "28633/28633 [==============================] - 0s - loss: 0.8056 - val_loss: 0.9085\n",
      "Epoch 154/200\n",
      "25000/28633 [=========================>....] - ETA: 0s - loss: 0.8061Epoch 00153: val_loss did not improve\n",
      "28633/28633 [==============================] - 0s - loss: 0.7993 - val_loss: 0.8345\n",
      "Epoch 155/200\n",
      "25000/28633 [=========================>....] - ETA: 0s - loss: 0.8456Epoch 00154: val_loss improved from 0.76763 to 0.74593, saving model to /tmp/logreg.hdf5\n",
      "28633/28633 [==============================] - 0s - loss: 0.8291 - val_loss: 0.7459\n",
      "Epoch 156/200\n",
      "25000/28633 [=========================>....] - ETA: 0s - loss: 0.8148Epoch 00155: val_loss did not improve\n",
      "28633/28633 [==============================] - 0s - loss: 0.8309 - val_loss: 0.8218\n",
      "Epoch 157/200\n",
      "25000/28633 [=========================>....] - ETA: 0s - loss: 0.8179Epoch 00156: val_loss did not improve\n",
      "28633/28633 [==============================] - 0s - loss: 0.8176 - val_loss: 0.7539\n",
      "Epoch 158/200\n",
      "25000/28633 [=========================>....] - ETA: 0s - loss: 0.8027Epoch 00157: val_loss did not improve\n",
      "28633/28633 [==============================] - 0s - loss: 0.7955 - val_loss: 0.7475\n",
      "Epoch 159/200\n",
      "25000/28633 [=========================>....] - ETA: 0s - loss: 0.8116Epoch 00158: val_loss did not improve\n",
      "28633/28633 [==============================] - 0s - loss: 0.8140 - val_loss: 0.8138\n",
      "Epoch 160/200\n",
      "25000/28633 [=========================>....] - ETA: 0s - loss: 0.7220Epoch 00159: val_loss did not improve\n",
      "28633/28633 [==============================] - 0s - loss: 0.7246 - val_loss: 0.8604\n",
      "Epoch 161/200\n",
      "25000/28633 [=========================>....] - ETA: 0s - loss: 0.7965Epoch 00160: val_loss did not improve\n",
      "28633/28633 [==============================] - 0s - loss: 0.7899 - val_loss: 0.7671\n",
      "Epoch 162/200\n",
      "25000/28633 [=========================>....] - ETA: 0s - loss: 0.7081Epoch 00161: val_loss did not improve\n",
      "28633/28633 [==============================] - 0s - loss: 0.7194 - val_loss: 0.9141\n",
      "Epoch 163/200\n",
      "25000/28633 [=========================>....] - ETA: 0s - loss: 0.8259Epoch 00162: val_loss did not improve\n",
      "28633/28633 [==============================] - 0s - loss: 0.8116 - val_loss: 0.8399\n",
      "Epoch 164/200\n",
      "25000/28633 [=========================>....] - ETA: 0s - loss: 0.7953Epoch 00163: val_loss did not improve\n",
      "28633/28633 [==============================] - 0s - loss: 0.7817 - val_loss: 0.7465\n",
      "Epoch 165/200\n",
      "25000/28633 [=========================>....] - ETA: 0s - loss: 0.8009Epoch 00164: val_loss did not improve\n",
      "28633/28633 [==============================] - 0s - loss: 0.7894 - val_loss: 0.7505\n",
      "Epoch 166/200\n",
      "25000/28633 [=========================>....] - ETA: 0s - loss: 0.8442Epoch 00165: val_loss did not improve\n",
      "\n",
      "Epoch 00165: reducing learning rate to 0.00010000000474974513.\n",
      "28633/28633 [==============================] - 0s - loss: 0.8299 - val_loss: 0.9003\n",
      "Epoch 167/200\n",
      "25000/28633 [=========================>....] - ETA: 0s - loss: 0.7705Epoch 00166: val_loss did not improve\n",
      "28633/28633 [==============================] - 0s - loss: 0.7714 - val_loss: 0.8010\n",
      "Epoch 168/200\n",
      "25000/28633 [=========================>....] - ETA: 0s - loss: 0.7283Epoch 00167: val_loss improved from 0.74593 to 0.73582, saving model to /tmp/logreg.hdf5\n",
      "28633/28633 [==============================] - 0s - loss: 0.7266 - val_loss: 0.7358\n",
      "Epoch 169/200\n",
      "25000/28633 [=========================>....] - ETA: 0s - loss: 0.7084Epoch 00168: val_loss did not improve\n",
      "28633/28633 [==============================] - 0s - loss: 0.7062 - val_loss: 0.7452\n",
      "Epoch 170/200\n",
      "25000/28633 [=========================>....] - ETA: 0s - loss: 0.7003Epoch 00169: val_loss did not improve\n",
      "28633/28633 [==============================] - 0s - loss: 0.6985 - val_loss: 0.7505\n",
      "Epoch 171/200\n",
      "25000/28633 [=========================>....] - ETA: 0s - loss: 0.6931Epoch 00170: val_loss improved from 0.73582 to 0.73468, saving model to /tmp/logreg.hdf5\n",
      "28633/28633 [==============================] - 0s - loss: 0.6933 - val_loss: 0.7347\n",
      "Epoch 172/200\n",
      "25000/28633 [=========================>....] - ETA: 0s - loss: 0.6892Epoch 00171: val_loss improved from 0.73468 to 0.73203, saving model to /tmp/logreg.hdf5\n",
      "28633/28633 [==============================] - 0s - loss: 0.6882 - val_loss: 0.7320\n",
      "Epoch 173/200\n",
      "25000/28633 [=========================>....] - ETA: 0s - loss: 0.6860Epoch 00172: val_loss did not improve\n",
      "28633/28633 [==============================] - 0s - loss: 0.6866 - val_loss: 0.7334\n",
      "Epoch 174/200\n",
      "25000/28633 [=========================>....] - ETA: 0s - loss: 0.6860Epoch 00173: val_loss improved from 0.73203 to 0.73098, saving model to /tmp/logreg.hdf5\n",
      "28633/28633 [==============================] - 0s - loss: 0.6856 - val_loss: 0.7310\n",
      "Epoch 175/200\n",
      "25000/28633 [=========================>....] - ETA: 0s - loss: 0.6853Epoch 00174: val_loss improved from 0.73098 to 0.73072, saving model to /tmp/logreg.hdf5\n",
      "28633/28633 [==============================] - 0s - loss: 0.6855 - val_loss: 0.7307\n",
      "Epoch 176/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000/28633 [=========================>....] - ETA: 0s - loss: 0.6851Epoch 00175: val_loss did not improve\n",
      "28633/28633 [==============================] - 0s - loss: 0.6850 - val_loss: 0.7320\n",
      "Epoch 177/200\n",
      "25000/28633 [=========================>....] - ETA: 0s - loss: 0.6854Epoch 00176: val_loss improved from 0.73072 to 0.73070, saving model to /tmp/logreg.hdf5\n",
      "28633/28633 [==============================] - 0s - loss: 0.6851 - val_loss: 0.7307\n",
      "Epoch 178/200\n",
      "25000/28633 [=========================>....] - ETA: 0s - loss: 0.6850Epoch 00177: val_loss improved from 0.73070 to 0.72992, saving model to /tmp/logreg.hdf5\n",
      "28633/28633 [==============================] - 0s - loss: 0.6851 - val_loss: 0.7299\n",
      "Epoch 179/200\n",
      "25000/28633 [=========================>....] - ETA: 0s - loss: 0.6842Epoch 00178: val_loss did not improve\n",
      "28633/28633 [==============================] - 0s - loss: 0.6849 - val_loss: 0.7303\n",
      "Epoch 180/200\n",
      "25000/28633 [=========================>....] - ETA: 0s - loss: 0.6834Epoch 00179: val_loss did not improve\n",
      "28633/28633 [==============================] - 0s - loss: 0.6848 - val_loss: 0.7311\n",
      "Epoch 181/200\n",
      "25000/28633 [=========================>....] - ETA: 0s - loss: 0.6850Epoch 00180: val_loss improved from 0.72992 to 0.72980, saving model to /tmp/logreg.hdf5\n",
      "28633/28633 [==============================] - 0s - loss: 0.6847 - val_loss: 0.7298\n",
      "Epoch 182/200\n",
      "25000/28633 [=========================>....] - ETA: 0s - loss: 0.6857Epoch 00181: val_loss did not improve\n",
      "28633/28633 [==============================] - 0s - loss: 0.6845 - val_loss: 0.7298\n",
      "Epoch 183/200\n",
      "25000/28633 [=========================>....] - ETA: 0s - loss: 0.6842Epoch 00182: val_loss improved from 0.72980 to 0.72915, saving model to /tmp/logreg.hdf5\n",
      "28633/28633 [==============================] - 0s - loss: 0.6845 - val_loss: 0.7292\n",
      "Epoch 184/200\n",
      "25000/28633 [=========================>....] - ETA: 0s - loss: 0.6841Epoch 00183: val_loss did not improve\n",
      "28633/28633 [==============================] - 0s - loss: 0.6843 - val_loss: 0.7294\n",
      "Epoch 185/200\n",
      "25000/28633 [=========================>....] - ETA: 0s - loss: 0.6850Epoch 00184: val_loss did not improve\n",
      "28633/28633 [==============================] - 0s - loss: 0.6844 - val_loss: 0.7300\n",
      "Epoch 186/200\n",
      "25000/28633 [=========================>....] - ETA: 0s - loss: 0.6841Epoch 00185: val_loss did not improve\n",
      "28633/28633 [==============================] - 0s - loss: 0.6844 - val_loss: 0.7293\n",
      "Epoch 187/200\n",
      "25000/28633 [=========================>....] - ETA: 0s - loss: 0.6840Epoch 00186: val_loss did not improve\n",
      "28633/28633 [==============================] - 0s - loss: 0.6842 - val_loss: 0.7293\n",
      "Epoch 188/200\n",
      "25000/28633 [=========================>....] - ETA: 0s - loss: 0.6850Epoch 00187: val_loss improved from 0.72915 to 0.72854, saving model to /tmp/logreg.hdf5\n",
      "28633/28633 [==============================] - 0s - loss: 0.6841 - val_loss: 0.7285\n",
      "Epoch 189/200\n",
      "25000/28633 [=========================>....] - ETA: 0s - loss: 0.6832Epoch 00188: val_loss did not improve\n",
      "28633/28633 [==============================] - 0s - loss: 0.6842 - val_loss: 0.7286\n",
      "Epoch 190/200\n",
      "25000/28633 [=========================>....] - ETA: 0s - loss: 0.6847Epoch 00189: val_loss did not improve\n",
      "28633/28633 [==============================] - 0s - loss: 0.6842 - val_loss: 0.7293\n",
      "Epoch 191/200\n",
      "25000/28633 [=========================>....] - ETA: 0s - loss: 0.6836Epoch 00190: val_loss did not improve\n",
      "28633/28633 [==============================] - 0s - loss: 0.6841 - val_loss: 0.7288\n",
      "Epoch 192/200\n",
      "25000/28633 [=========================>....] - ETA: 0s - loss: 0.6836Epoch 00191: val_loss improved from 0.72854 to 0.72827, saving model to /tmp/logreg.hdf5\n",
      "28633/28633 [==============================] - 0s - loss: 0.6838 - val_loss: 0.7283\n",
      "Epoch 193/200\n",
      "25000/28633 [=========================>....] - ETA: 0s - loss: 0.6841Epoch 00192: val_loss did not improve\n",
      "28633/28633 [==============================] - 0s - loss: 0.6839 - val_loss: 0.7284\n",
      "Epoch 194/200\n",
      "25000/28633 [=========================>....] - ETA: 0s - loss: 0.6827Epoch 00193: val_loss did not improve\n",
      "28633/28633 [==============================] - 0s - loss: 0.6837 - val_loss: 0.7284\n",
      "Epoch 195/200\n",
      "25000/28633 [=========================>....] - ETA: 0s - loss: 0.6840Epoch 00194: val_loss improved from 0.72827 to 0.72802, saving model to /tmp/logreg.hdf5\n",
      "28633/28633 [==============================] - 0s - loss: 0.6838 - val_loss: 0.7280\n",
      "Epoch 196/200\n",
      "25000/28633 [=========================>....] - ETA: 0s - loss: 0.6835Epoch 00195: val_loss did not improve\n",
      "28633/28633 [==============================] - 0s - loss: 0.6839 - val_loss: 0.7284\n",
      "Epoch 197/200\n",
      "25000/28633 [=========================>....] - ETA: 0s - loss: 0.6830Epoch 00196: val_loss improved from 0.72802 to 0.72794, saving model to /tmp/logreg.hdf5\n",
      "28633/28633 [==============================] - 0s - loss: 0.6837 - val_loss: 0.7279\n",
      "Epoch 198/200\n",
      "25000/28633 [=========================>....] - ETA: 0s - loss: 0.6833Epoch 00197: val_loss did not improve\n",
      "28633/28633 [==============================] - 0s - loss: 0.6836 - val_loss: 0.7281\n",
      "Epoch 199/200\n",
      "25000/28633 [=========================>....] - ETA: 0s - loss: 0.6838Epoch 00198: val_loss improved from 0.72794 to 0.72787, saving model to /tmp/logreg.hdf5\n",
      "\n",
      "Epoch 00198: reducing learning rate to 1.0000000474974514e-05.\n",
      "28633/28633 [==============================] - 0s - loss: 0.6835 - val_loss: 0.7279\n",
      "Epoch 200/200\n",
      "25000/28633 [=========================>....] - ETA: 0s - loss: 0.6831Epoch 00199: val_loss improved from 0.72787 to 0.72757, saving model to /tmp/logreg.hdf5\n",
      "28633/28633 [==============================] - 0s - loss: 0.6832 - val_loss: 0.7276\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f6670221ac8>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "saver_logreg = keras.callbacks.ModelCheckpoint(filepath='/tmp/logreg.hdf5', monitor='val_loss', verbose=1, save_best_only=True)\n",
    "lr_decreaser_logreg = keras.callbacks.ReduceLROnPlateau(monitor='val_loss', patience=10, factor=0.1, epsilon=0.001, verbose=1)\n",
    "\n",
    "logreg.fit(X_train, Y_D1_train, validation_data = (X_test, Y_D1_test),\n",
    "           verbose=1, batch_size=5000, epochs=200, \n",
    "           callbacks=[saver_logreg, lr_decreaser_logreg])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Учим CNN на лейблах - предиктах логрегрессии"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 28633 samples, validate on 12272 samples\n",
      "Epoch 1/1\n",
      "28608/28633 [============================>.] - ETA: 0s - loss: 1.3824Epoch 00000: val_loss improved from inf to 0.68949, saving model to /tmp/cnn_old.hdf5\n",
      "28633/28633 [==============================] - 56s - loss: 1.3817 - val_loss: 0.6895\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f6670054400>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "saver_cnn = keras.callbacks.ModelCheckpoint(filepath='/tmp/cnn_old.hdf5', monitor='val_loss', verbose=1, save_best_only=True)\n",
    "lr_decreaser_cnn = keras.callbacks.ReduceLROnPlateau(monitor='val_loss', patience=3, factor=0.5, epsilon=0.001, verbose=1)\n",
    "\n",
    "logreg.load_weights('/tmp/logreg.hdf5')\n",
    "\n",
    "cnn.fit(X_train, logreg.predict(X_train), validation_data = (X_test, Y_D1_test),\n",
    "                    verbose=1, epochs=1,\n",
    "                    callbacks=[saver_cnn, lr_decreaser_cnn])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "логрег + кнн тренировались столько:  105.20178413391113\n"
     ]
    }
   ],
   "source": [
    "print('логрег + кнн тренировались столько: ', time() - t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cnn_pure = Sequential()\n",
    "cnn_pure.add(Conv2D(8, kernel_size=(3, 3),\n",
    "                 activation='relu',\n",
    "                 input_shape=input_shape))\n",
    "cnn_pure.add(Conv2D(16, (3, 3), activation='relu'))\n",
    "cnn_pure.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "cnn_pure.add(Dropout(0.25))\n",
    "cnn_pure.add(Flatten())\n",
    "cnn_pure.add(Dense(64, activation='relu'))\n",
    "cnn_pure.add(Dropout(0.5))\n",
    "cnn_pure.add(Dense(1, activation='sigmoid'))\n",
    "cnn_pure.compile(loss=keras.losses.binary_crossentropy,\n",
    "                        optimizer=keras.optimizers.Adam())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "t0=time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 28633 samples, validate on 12272 samples\n",
      "Epoch 1/2\n",
      "28608/28633 [============================>.] - ETA: 0s - loss: 5.8217Epoch 00000: val_loss improved from inf to 0.69302, saving model to /tmp/cnn_new.hdf5\n",
      "28633/28633 [==============================] - 56s - loss: 5.8172 - val_loss: 0.6930\n",
      "Epoch 2/2\n",
      "28608/28633 [============================>.] - ETA: 0s - loss: 0.6927Epoch 00001: val_loss improved from 0.69302 to 0.69212, saving model to /tmp/cnn_new.hdf5\n",
      "28633/28633 [==============================] - 57s - loss: 0.6927 - val_loss: 0.6921\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f66701cb198>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "saver_cnn_pure = keras.callbacks.ModelCheckpoint(filepath='/tmp/cnn_new.hdf5', monitor='val_loss', verbose=1, save_best_only=True)\n",
    "lr_decreaser_cnn_pure = keras.callbacks.ReduceLROnPlateau(monitor='val_loss', patience=3, factor=0.5, epsilon=0.001, verbose=1)\n",
    "\n",
    "\n",
    "cnn_pure.fit(X_train, Y_D1_train, validation_data = (X_test, Y_D1_test),\n",
    "                    verbose=1, epochs=2,\n",
    "                    callbacks=[saver_cnn_pure, lr_decreaser_cnn_pure])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "чистая кнн тренировалась столько:  115.10945248603821\n"
     ]
    }
   ],
   "source": [
    "print('чистая кнн тренировалась столько: ', time() - t0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Предиктим D0 vs D1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cnn.load_weights('/tmp/cnn_old.hdf5')\n",
    "cnn_pure.load_weights('/tmp/cnn_new.hdf5')\n",
    "logreg.load_weights('/tmp/logreg.hdf5')\n",
    "Y_pred_cnn = cnn.predict(x)\n",
    "Y_pred_logreg = logreg.predict(x)\n",
    "Y_pred_cnn_pure = cnn_pure.predict(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Смотрим на распределения вероятностей для класса C_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYAAAAD1CAYAAABZXyJ5AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f66702423c8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "mask = [i for i,elem in enumerate(y_C1) if elem == 1]\n",
    "plt.hist(Y_pred_logreg[mask], bins=10, alpha=0.1, label='logreg')\n",
    "plt.hist(Y_pred_cnn[mask], bins=12, alpha=0.5, label='cnn')\n",
    "plt.hist(Y_pred_cnn_pure[mask], bins=12, alpha=0.5, label='cnn_pure')\n",
    "plt.legend(loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Смотрим на распределения вероятностей для класса C_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXkAAAD1CAYAAAC1BoUqAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nMIW8iIiFKeRFRCxMIS8iYmEKeRERC/t/9fZAA1jKKr4AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f66645e9438>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "mask = [i for i,elem in enumerate(y_C1) if elem == 0]\n",
    "plt.hist(Y_pred_logreg[mask], bins=10, alpha=0.1, label='logreg')\n",
    "plt.hist(Y_pred_cnn[mask], bins=10, alpha=0.5, label='cnn')\n",
    "plt.hist(Y_pred_cnn_pure[mask], bins=10, alpha=0.5, label='cnn_pure')\n",
    "plt.legend(loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYAAAAD1CAYAAABZXyJ5AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f66704db470>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "mask = [i for i,elem in enumerate(y_C1)]\n",
    "plt.hist(Y_pred_logreg[mask], bins=160, alpha=0.1, label='logreg')\n",
    "plt.hist(Y_pred_cnn[mask], bins=60, alpha=1, label='cnn')\n",
    "plt.hist(Y_pred_cnn_pure[mask], bins=60, alpha=1, label='cnn_pure')\n",
    "plt.legend(loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Рокауки на задаче C0 vs C1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logreg rocauc: 0.889860990557\n",
      "cnn rocauc: 0.976458356035\n",
      "cnn_pure rocauc: 0.873324302761\n"
     ]
    }
   ],
   "source": [
    "print('logreg rocauc:', rocauc(Y_C1_test, D1_problem_to_C1_problem(logreg.predict(X_test)).flat))\n",
    "print('cnn rocauc:', rocauc(Y_C1_test, D1_problem_to_C1_problem(cnn.predict(X_test)).flat))\n",
    "print('cnn_pure rocauc:', rocauc(Y_C1_test, D1_problem_to_C1_problem(cnn_pure.predict(X_test)).flat))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
